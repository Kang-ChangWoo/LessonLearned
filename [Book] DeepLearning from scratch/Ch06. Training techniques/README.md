# 6. Training techniques
## a. Lesson learned
- Neural network(신경망) 학습의 목적은 Loss function(손실 함수)의 값을 가능한 한 낮추는 Parameter(매개변수)를 찾는 것이다.

- 이러한 과정을 Optimization(최적화)라고 한다.
- SGD는 단순하지만 효과적이다.  이와 다른 최적화 기법도 존재한다.

- SGD의 단점: 비등방성(anisotropy) 함수라는 점.

- Momentum: SGD에 속도 개념을 추가하여 비교적 빠르게 최적해를 찾아간다.

- AdaGrad: adaptive learning rate를 이용한다.

- Adam

## b. Programming
- 

# Futher study
1. Sigmoid 함수의 미분하는 방법
2. Adam의 구현?
3. SGD, 모멘텀, AdaGrad, Adam 중 장단점이 무엇일까? (책 기준 많은 연구에서 SGD를 쓰고 모멘텀과 AdaGrad도 가치 있지만, Adam을 많이 쓰는 듯.
4. 6.1.8 그리기

# Self-made Question


